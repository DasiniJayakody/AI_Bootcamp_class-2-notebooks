{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854488f2",
   "metadata": {},
   "source": [
    "# üöÄ OpenAI Python SDK 101\n",
    "\n",
    "In this notebook we‚Äôll learn how to interact with Large Language Models (LLMs) directly using the **OpenAI Python SDK**.  \n",
    "This is the **first time** we‚Äôre exploring API interactions, so we‚Äôll build up gradually:\n",
    "\n",
    "1. **Initialize** the client with your API key.  \n",
    "2. **Minimal call** to the API (Responses API).  \n",
    "3. Use **Chat Completions** for system + user roles.  \n",
    "4. Explore **temperature** (randomness) and **top_p** (nucleus sampling).  \n",
    "5. Add **system prompts** to guide behavior.  \n",
    "6. Try **streaming tokens** (like ChatGPT typing).  \n",
    "7. Get **JSON/structured outputs** with schemas.  \n",
    "8. Handle **errors, timeouts, and retries** gracefully.\n",
    "\n",
    "By the end, you‚Äôll know how to **call an LLM safely and flexibly** using just the OpenAI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "844e5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf239a",
   "metadata": {},
   "source": [
    "### API key\n",
    "- Set your OpenAI API key as an environment variable:  \n",
    "  `export OPENAI_API_KEY=\"sk-...\"` (macOS/Linux) or `setx OPENAI_API_KEY \"sk-...\"` (Windows, new terminal required).  \n",
    "- In Colab: use `os.environ[\"OPENAI_API_KEY\"] = \"...\"` (for demos only).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b03c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Python SDK v1 style\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c9cd8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature controls the randomness of predictions by scaling logit probabilities, while top-p (nucleus) sampling limits choices to the most probable tokens that cumulatively meet a specified probability threshold.\n"
     ]
    }
   ],
   "source": [
    "# Minimal \"Responses API\" call (recommended by OpenAI for new projects)\n",
    "# Docs: https://platform.openai.com/docs/guides/text  and Responses vs Chat Completions\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",  # choose any available text-capable model\n",
    "    input=\"In one sentence, explain the difference between temperature and top_p for sampling.\"\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23364e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_07056a317840d2d30068f32ef5b3e08193b283c91c1e3b5838', created_at=1760767733.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_07056a317840d2d30068f32ef72650819382acc98e20ea9f6f', content=[ResponseOutputText(annotations=[], text='Temperature controls the randomness of predictions by scaling logit probabilities, while top-p (nucleus) sampling limits choices to the most probable tokens that cumulatively meet a specified probability threshold.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=22, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=37, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=59), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff892c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Definition**: Overfitting occurs when a model learns the training data too well, capturing noise and outliers instead of the underlying patterns, resulting in poor generalization to new, unseen data.\n",
      "\n",
      "- **Symptoms**: Indicators of overfitting include a significantly lower training error compared to validation or test error, and high model complexity relative to the amount of training data.\n",
      "\n",
      "- **Prevention Strategies**: Techniques to prevent overfitting include using simpler models, implementing regularization methods, employing cross-validation, and using techniques like dropout in neural networks.\n"
     ]
    }
   ],
   "source": [
    "# Using Chat Completions (still widely used & supported)\n",
    "chat = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise teaching assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 bullet points about overfitting.\"}\n",
    "    ]\n",
    ")\n",
    "print(chat.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d05319c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CRuPY2Egt90G5YE11SGTmhm3A2Hvd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='- **Definition**: Overfitting occurs when a model learns the training data too well, capturing noise and outliers instead of the underlying patterns, resulting in poor generalization to new, unseen data.\\n\\n- **Symptoms**: Indicators of overfitting include a significantly lower training error compared to validation or test error, and high model complexity relative to the amount of training data.\\n\\n- **Prevention Strategies**: Techniques to prevent overfitting include using simpler models, implementing regularization methods, employing cross-validation, and using techniques like dropout in neural networks.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760767736, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=112, prompt_tokens=29, total_tokens=141, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5dddecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use slicing to reverse a string in Python. Here's an example:\n",
      "\n",
      "```python\n",
      "# Input string\n",
      "original_string = \"hello, world!\"\n",
      "\n",
      "# Reversed string\n",
      "reversed_string = original_string[::-1]\n",
      "\n",
      "print(reversed_string)  # Output: !dlrow ,olleh\n",
      "```\n",
      "\n",
      "This code creates a new string that is the reverse of `original_string` by utilizing slice notation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Python tutor who answers with short code examples.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Show how to reverse a string in Python.\"}\n",
    "]\n",
    "r = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, temperature=2) #High temperature for more creative output\n",
    "print(r.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "553e975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a charming little village, there lived a cat named Whiskers and a dog named Bruno. Whiskers was a sleek gray tabby with emerald green eyes, known for her grace and cunning. Bruno, on the other hand, was a large, friendly golden retriever with a heart as big as his paws.\n",
      "\n",
      "Despite their differences, the two animals enjoyed a curious friendship. They lived next door to each other; Whiskers claimed the warm windowsill of her owner's home, while Bruno basked in the sun on his porch. Every day, they would meet at the split fence that separated their yards, exchanging stories and planning their next adventure.\n",
      "\n",
      "One sunny afternoon, as the birds chirped merrily, Whiskers proposed a daring idea. ‚ÄúLet‚Äôs go on an exploration to the old mill at the edge of the woods! I‚Äôve heard tales of treasures hidden there,‚Äù she said, her eyes sparkling with excitement.\n",
      "\n",
      "Bruno‚Äôs tail wagged enthusiastically. ‚ÄúI‚Äôm in! How do we get there?‚Äù\n",
      "\n",
      "With a flick of her tail, Whiskers devised a route through the gardens and down the winding path that led to the woods. Together, they set off, Whiskers leading the way with her agile leaps, while Bruno lumbered after her, his playful bark echoing through the trees.\n",
      "\n",
      "As they approached the mill, a sense of adventure tingled in the air. The old structure loomed ahead, cloaked in vines and secrets. The door creaked open at their approach, revealing a dusty interior filled with cobwebs and scattered wooden beams. Whiskers ducked inside, her keen eyes scanning the dark corners. Bruno hesitated, unsure of the looming shadows.\n",
      "\n",
      "‚ÄúCome on, Bruno! There‚Äôs nothing to be afraid of!‚Äù Whiskers called, her voice echoing in the silence. She spotted a glint beneath a wooden floorboard and scampered over.\n",
      "\n",
      "With a nudge from Bruno, she pried the board open, revealing a small, rusted tin box. Together, they tugged it free, excitement bubbling between them. What could it be? They opened it cautiously, revealing a collection of colorful marbles, sparkling as they caught the light.\n",
      "\n",
      "‚ÄúOh! Look at these!‚Äù Whiskers said, her voice filled with delight. ‚ÄúThey‚Äôre beautiful!‚Äù\n",
      "\n",
      "Bruno‚Äôs nose pressed closer to the marbles, and then he let out a joyful bark. ‚ÄúThey‚Äôre perfect for playing fetch! Let‚Äôs take them home!‚Äù\n",
      "\n",
      "They spent the afternoon rolling the marbles back and forth, turning the dusty old mill into their own personal playground. Whiskers would swat them with her paw, sending them tumbling toward Bruno, who would chase after them, his tail wagging like a flag in the breeze. The laughter echoed through the woods as they played until the sun dipped low.\n",
      "\n",
      "As they finally made their way back home, marbles safely nestled under Whiskers‚Äô paw, they exchanged contented glances. Their friendship, built on trust and adventure, shone brighter than any treasure they could have ever found.\n",
      "\n",
      "Under the soft glow of the evening stars, Whiskers curled up on her windowsill, and Bruno lay on his porch, dreaming of more adventures to come. In their hearts, they knew that as long as they had each other, every day would be filled with magic‚Äîreal or imagined. And so, in their little corner of the world, a cat and a dog shared their days, united in friendship, forever ready to explore the unknown together.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import stdout\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a short story about a cat and a dog.\"}],\n",
    "    temperature=1,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    if hasattr(event, \"choices\"):\n",
    "        delta = event.choices[0].delta\n",
    "        if delta and delta.content:\n",
    "            stdout.write(delta.content)\n",
    "stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6e2b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"item\": \"Rice\",\n",
      "        \"quantity\": 3\n",
      "    },\n",
      "    {\n",
      "        \"item\": \"Dhal\",\n",
      "        \"quantity\": 4\n",
      "    },\n",
      "    {\n",
      "        \"item\": \"Biscuits\",\n",
      "        \"quantity\": 3\n",
      "    },\n",
      "    {\n",
      "        \"item\": \"Sugar\",\n",
      "        \"quantity\": 2\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"I have bought 3 kg of Rice, 4 kg of Dhal, 3 packets of Biscuits, 2 kg of Suger, Format this as a list of json objects with each JSON object in the format:\n",
    "{\n",
    "\"item\": \"<item name>\", \n",
    "\"quantity\": <quantity in kg or packets> \n",
    "}\n",
    "\n",
    "DO NOT include anything else in the response\"\"\"\n",
    "\n",
    "\n",
    "completion = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=1,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8fe589e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "\n",
    "items = json.loads(response)\n",
    "type(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843dd69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rice\n",
      "Dhal\n",
      "Biscuits\n",
      "Sugar\n"
     ]
    }
   ],
   "source": [
    "for item in items:\n",
    "    print(item[\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7efd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary(topic='Transformers in NLP', key_points=['Transformers utilize self-attention mechanisms to weigh the importance of different words in a sentence, allowing for better context understanding.', 'They enable parallel processing of data, significantly speeding up training times compared to traditional sequential models like RNNs.', 'Transformers have led to the development of powerful pre-trained models (e.g., BERT, GPT) that can be fine-tuned for various NLP tasks, achieving state-of-the-art results.'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    topic: str\n",
    "    key_points: List[str]\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    response_format=Summary,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Topic: Transformers in NLP. Give 3 key points.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "parsed = completion.choices[0].message.parsed\n",
    "parsed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
